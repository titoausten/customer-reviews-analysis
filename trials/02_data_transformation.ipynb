{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Personal AI Projects\\\\FORAGE JOB SIMULATIONS\\\\British Airline Data Science Virtual Internship\\\\customer-reviews-analysis\\\\trials'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Personal AI Projects\\\\FORAGE JOB SIMULATIONS\\\\British Airline Data Science Virtual Internship\\\\customer-reviews-analysis'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### src/reviewAnalyser/entity/config_entity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    source_data_path: str\n",
    "    local_data_path: Path\n",
    "    feature_for_new_feature: str\n",
    "    feature_split_string: str\n",
    "    new_feature: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### src/reviewAnalyser/config/configuration.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.reviewAnalyzer.constants import *\n",
    "from src.reviewAnalyzer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_data_path=config.source_data_path,\n",
    "            local_data_path=config.local_data_path,\n",
    "            feature_for_new_feature=config.feature_for_new_feature,\n",
    "            feature_split_string=config.feature_split_string,\n",
    "            new_feature=config.new_feature\n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### src/reviewAnalyser/components/data_transformation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Tito\n",
      "[nltk_data]     Osadebey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Tito\n",
      "[nltk_data]     Osadebey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Tito\n",
      "[nltk_data]     Osadebey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src.reviewAnalyzer import logger\n",
    "from src.reviewAnalyzer.utils.common import get_size\n",
    "from src.reviewAnalyzer.entity.config_entity import DataTransformationConfig\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.file = self.config.source_data_path\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.df = self.load_data()\n",
    "\n",
    "    \n",
    "    def load_data(self):\n",
    "        # Load data\n",
    "        print(f\"Loading source data file...\")\n",
    "        df = pd.read_csv(self.file, index_col=0)\n",
    "        logger.info(f\"Data file ({self.file}) loaded\")\n",
    "        return df\n",
    "        \n",
    "\n",
    "    def create_feature(self):      \n",
    "        # df = self.load_data()\n",
    "        # Create a column in the dataframe based on what another feature contains\n",
    "        print(f\"Creating feature ({self.config.new_feature}) from ({self.config.feature_for_new_feature})\")\n",
    "        self.df[self.config.new_feature] = self.df[self.config.feature_for_new_feature].str.contains(self.config.feature_split_string)\n",
    "        logger.info(f\"Feature ({self.config.new_feature}) created\")\n",
    "        return self.df\n",
    "        \n",
    "\n",
    "    def clean_data(self, document):\n",
    "        # df = self.create_feature()\n",
    "        document = document.split('|')[1]\n",
    "        document = re.sub('[^a-zA-Z]',' ', str(document))\n",
    "        document = document.lower()\n",
    "        document = document.split()\n",
    "        # Fixing contractions e.g don't, won't etc.\n",
    "        #document = contractions.fix(document)\n",
    "        document = \" \".join(document)\n",
    "        return document\n",
    "    \n",
    "\n",
    "    def process_data(self, document):\n",
    "        tokens = word_tokenize(document)\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        tag_map = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "        new_tagged_tokens = []\n",
    "        for word, tag in tagged_tokens:\n",
    "            if word.lower() not in set(stopwords.words('english')):\n",
    "                new_tagged_tokens.append(tuple([word, tag_map.get(tag[0])]))\n",
    "        return new_tagged_tokens\n",
    "\n",
    "\n",
    "    def lemmatiza(self, document):\n",
    "        #df = self.process_data()\n",
    "        lemmatized_text = \" \"\n",
    "        for word, tag in document:\n",
    "            if not tag:\n",
    "                lemmatize = word\n",
    "                lemmatized_text = lemmatized_text + \" \" + lemmatize\n",
    "            else:\n",
    "                lemmatize = self.lemmatizer.lemmatize(word, pos=tag)\n",
    "                lemmatized_text = lemmatized_text + \" \" + lemmatize\n",
    "        return lemmatized_text\n",
    "\n",
    "\n",
    "    def save_data(self, document: pd.DataFrame):\n",
    "        document.to_csv(self.config.local_data_path)\n",
    "        logger.info(f\"Preprocessed file location: {self.config.local_data_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### src/reviewAnalyser/pipeline/stage_01_data_ingestion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformationPipeline:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def main(self):  \n",
    "        config = ConfigurationManager()\n",
    "        data_transformation_config = config.get_data_transformation_config()\n",
    "\n",
    "        if not os.path.exists(data_transformation_config.local_data_path):\n",
    "            data_transformation = DataTransformation(config=data_transformation_config)\n",
    "            new_data = data_transformation.create_feature()\n",
    "            print(f\"Cleaning data file...\")\n",
    "            new_data['cleaned_reviews'] = new_data['reviews'].apply(data_transformation.clean_data)\n",
    "            logger.info(f\"Data file cleaned\")\n",
    "            logger.info(f\"Part of Speech Tagging...\")\n",
    "            print(f\"Tokenizing...\")\n",
    "            print(f\"Part of Speech Tagging...\")\n",
    "            print(f\"Removing stopwords...\")\n",
    "            new_data['pos_tagged'] = new_data['cleaned_reviews'].apply(data_transformation.process_data)\n",
    "            logger.info(f\"Part of Speech Tagging done.\")\n",
    "            print(f\"Word lemmatizing...\")\n",
    "            new_data['corpus'] = new_data['pos_tagged'].apply(data_transformation.lemmatiza)\n",
    "            logger.info(f\"Word lemmatizing done and corpus feature created.\")\n",
    "            data_transformation.save_data(new_data)\n",
    "        else:\n",
    "            logger.info(f\"File already exists of size: {get_size(Path(data_transformation_config.local_data_path))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.reviewAnalyzer.exceptions import CustomException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-09 20:36:42,255: INFO: 3954784042: *******************]\n",
      "[2024-07-09 20:36:42,256: INFO: 3954784042: >>>>>> Data Transformation Stage started <<<<<<]\n",
      "[2024-07-09 20:36:42,267: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-07-09 20:36:42,271: INFO: common: created directory at: artifacts]\n",
      "[2024-07-09 20:36:42,273: INFO: common: created directory at: artifacts/data_transformation]\n",
      "Loading source data file...\n",
      "[2024-07-09 20:36:42,462: INFO: 462053332: Data file (artifacts/data_ingestion/data.csv) loaded]\n",
      "Creating feature (verified) from (reviews)\n",
      "[2024-07-09 20:36:42,481: INFO: 462053332: Feature (verified) created]\n",
      "Cleaning data file...\n",
      "[2024-07-09 20:36:42,629: INFO: 1634043365: Data file cleaned]\n",
      "[2024-07-09 20:36:42,631: INFO: 1634043365: Part of Speech Tagging...]\n",
      "Tokenizing...\n",
      "Part of Speech Tagging...\n",
      "Removing stopwords...\n",
      "[2024-07-09 20:38:01,960: INFO: 1634043365: Part of Speech Tagging done.]\n",
      "Word lemmatizing...\n",
      "[2024-07-09 20:38:02,361: INFO: 1634043365: Word lemmatizing done and corpus feature created.]\n",
      "[2024-07-09 20:38:02,573: INFO: 462053332: Preprocessed file location: artifacts/data_transformation/data-v1.0.csv]\n",
      "[2024-07-09 20:38:02,577: INFO: 3954784042: >>>>>> Data Transformation Stage completed <<<<<<]\n",
      "\n",
      "[x==========x]\n"
     ]
    }
   ],
   "source": [
    "STAGE_NAME = \"Data Transformation Stage\"\n",
    "\n",
    "try: \n",
    "   logger.info(f\"*******************\")\n",
    "   logger.info(f\">>>>>> {STAGE_NAME} started <<<<<<\")\n",
    "   data_transformation = DataTransformationPipeline()\n",
    "   data_transformation.main()\n",
    "   logger.info(f\">>>>>> {STAGE_NAME} completed <<<<<<]\\n\\n[x==========x\")\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f7924c4c56b083e0e50eadfe7ef592a7a8ef70df33a0047f82280e6be1afe15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
